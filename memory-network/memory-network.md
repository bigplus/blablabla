0. Attention
加权求和
content-based addressing
注意机制可能早就有思萌芽，但最早正式提出这一概念是在2014年，  Dzmitry Bahdanau，KyungHyun Cho 和 Yoshua Bengio的一篇论文Neural Machine Translation by Jointly Learning to Align and Translate 里提到了attention mechanism这个词。从题目也看到了，作者提出这一概念的时候并没有突出「attention]，这只是一种「jointly align and translate」的技术。原文第一次提到「attention」的句子是：『 Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to』。可以感觉到作者那时并没有意识到「attention mechanism」会火起来。


http://blog.csdn.net/malefactor/article/details/50550211
https://zhuanlan.zhihu.com/p/28054589
https://www.zhihu.com/question/36591394
https://www.zhihu.com/question/61077555?rf=61095891
http://blog.csdn.net/malefactor/article/details/50550211
http://blog.csdn.net/joshuaxx316/article/details/70665388
http://geek.csdn.net/news/detail/50558
http://xueqiu.com/3426965578/88317304
http://blog.csdn.net/qq_26609915/article/details/52086772

Bengio
http://news.hexun.com/2016-05-17/183891218.html

http://memect.com/

1. MN
content-based addressing + location-based addressing

https://arxiv.org/pdf/1410.3916v11.pdf
  http://blog.csdn.net/xizero00/article/details/51181948
  http://m.blog.csdn.net/wang735019/article/details/53909079
  http://blog.csdn.net/u011274209/article/details/53384232?ref=myread

csdn后面的相关文章, 很好
https://github.com/npow/MemNN

2. End to End Memory Network
https://arxiv.org/pdf/1503.08895.pdf
  http://blog.csdn.net/xizero00/article/details/51182003
  http://blog.csdn.net/u014300008/article/details/52794821
  https://www.zhihu.com/question/56479906
